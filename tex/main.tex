\documentclass[11pt,twoside]{report}
\usepackage{geometry}
\usepackage{enumerate}
\usepackage{latexsym,booktabs}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[singlespacing]{setspace}
% ===== Defaults^
\usepackage{amsthm}
\usepackage{hyperref}           % hyperlinks
\usepackage{array}              % for \newcolumntype macro
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{mathtools}

\newcolumntype{C}{>{$}c<{$}}    % math-mode version of "l" column type
\newcolumntype{L}{>{$}l<{$}} 

\newcommand{\A}{\mathcal{A}} %% I am lazy
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\binary}{\left\{0,1\right\}}

\DeclareMathOperator{\MEB}{MEB}
\DeclareMathOperator{\MEBwO}{MEBwO}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\theoremstyle{definition}
% ==== Mine^

\geometry{a4paper,left=2cm,right=2.0cm, top=2cm, bottom=2.0cm}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem*{remark}{Remark}
\numberwithin{theorem}{section}
\numberwithin{definition}{section}
\numberwithin{lemma}{section}
\numberwithin{proposition}{section}
\numberwithin{equation}{section}


\begin{document}
\pagestyle{empty}

% =============================================================================
% Title page
% =============================================================================
\begin{titlepage}
\vspace*{.5em}
\center
\textbf{\large{The School of Mathematics}} \\
\vspace*{1em}
\begin{figure}[!h]
\centering
\includegraphics[width=180pt]{CentredLogoCMYK.jpg}
\end{figure}
\vspace{2em}
\textbf{\Huge{Minimum Enclosing Balls with Outliers}}\\[2em]
\textbf{\LARGE{by}}\\
\vspace{2em}
\textbf{\LARGE{Thomas Holmes}}\\
\vspace{6.5em}
\Large{Dissertation Presented for the Degree of\\
MSc in Operational Research with Computational Optimization}\\
\vspace{6.5em}
\Large{August 2021}\\
\vspace{3em}
\Large{Supervised by\\Dr E. Alper Yıldırım}
\vfill
\end{titlepage}

\cleardoublepage

% =============================================================================
% Abstract, acknowledgments, and own work declaration
% =============================================================================
\vspace*{10mm}
\begin{center}
\textbf{\huge{Abstract}}
\end{center}

Here comes your abstract ...

\clearpage
\vspace*{10mm}
\begin{center}
\textbf{\huge{Acknowledgments}}
\end{center}

Here come your acknowledgments ...

\clearpage

\vspace*{10mm}
\begin{center}
\textbf{\huge{Own Work Declaration}}
\end{center}
\vspace*{20mm}

\noindent I declare that this thesis was composed by myself and that the work contained therein is my own, except where explicitly stated otherwise in the text.

\vspace*{10mm}

\begin{flushright}
\textit{(Thomas Holmes)}
\end{flushright}

\cleardoublepage



% =============================================================================
% Table of contents, tables, and pictures (if applicable)
% =============================================================================
\pagestyle{plain}
\setcounter{page}{1}
\pagenumbering{Roman}

\tableofcontents
\clearpage
\listoftables
\listoffigures
\cleardoublepage

\pagenumbering{arabic}
\setcounter{page}{1}

\nocite{*}
\bibliographystyle{abbrv}
\clearpage
% =============================================================================
% Main body
% =============================================================================

% =============================================================================
% Chapter 1
% =============================================================================
\chapter{Introduction}
\section{Motivation}
\section{Literature Review}\label{lit review}
\section{Outline}

% =============================================================================
% Chapter 2
% =============================================================================
\chapter{Exact Methods}\label{exact}
\section{Preliminaries}
In this paper we shall denote our data set of finite vectors as $\mathcal{A} = \left\{a^1,\ldots,a^n\right\}\subseteq\mathbb{R}^d$ for $n,d\in\mathbb{N}$. The center of a ball is represented by a vector $c\in\mathbb{R}^d$, the radius a scalar $r\in\mathbb{R}$, and the squared radius $\gamma=r^2$. We denote the percentage of inliers for a minimum enclosing ball with outliers by $\eta\in(0,1]$, i.e. if $\eta=0.9$ then we seek a ball which covers $90\%$ of our data.

\begin{definition}
Let $c\in\mathbb{R}^n$ and $r\in\mathbb{R}$. Then the \textit{ball} with center $c$ and radius $r$ is the set
\begin{equation*}
    B(c;r) = \left\{x\in\mathbb{R}^n : \norm{x-c} \leq r\right\}
\end{equation*}
where $\norm{\cdot}:L\to\mathbb{R}$ denotes the standard Euclidean norm on a vector space $L$ (in this paper, $L=\mathbb{R}^n$).
\end{definition}

\begin{definition}
The \textit{minimum enclosing ball} of $\A$, denoted by $\MEB(\A)$, is the ball $B(c^*,r^*)$ where $\A\subseteq B(c^*,r^*)$ and if any $B(c,r)$ exists such that $\mathcal{A}\subseteq B(c,r)$ then $r^*\leq r$.
\end{definition}

\begin{theorem}
For a given set $\A$, $\MEB(\A)$ exists and is unique.
\end{theorem}
\begin{proof}
See \cite[page 5]{two-algorithms}.
\end{proof}

\begin{proposition}\label{adding data}
Let $a'\in\mathbb{R}^n$ where $a'\notin\A$. Suppose $B(c,r)=\MEB(\A)$ and $B(c',r')=\MEB(\A\cap\left\{a'\right\})$. Then $r\leq r'$.
\end{proposition}
\begin{proof}
Clearly we have $\A\subseteq\A\cap\left\{a'\right\}$, then since $\A\cap\left\{a'\right\}\subseteq B(c',r')$ we have $\mathcal{A}\subseteq B(c',r')$ by transitivity. Thus $r\leq r'$ with equality only if $a'\in B(c,r)$ by uniqueness.
\end{proof}
\begin{remark}
This proposition tells us that if we have the MEB of a data set $\mathcal{A}$, then adding a new point to $\mathcal{A}$ which is not inside $\MEB(\A)$ yields a new MEB with larger radius.
\end{remark}

\begin{definition}[{{\cite[page 2]{core-sets}}}]
Let $r^*$ be the radius of $\MEB(\A)$. A ball $B(c;(1+\epsilon)r)$ is a \textit{$(1+\epsilon)$-approximation} of $\MEB(\A)$ if $r\leq r^*$ and $\mathcal{A}\subseteq B(c;(1+\epsilon)r)$.
\end{definition}



\section{Minimum Enclosing Ball}
\begin{definition}\label{meb}
The optimization model formulation of the Minimum Enclosing Ball (MEB) problem is as follows:
\begin{center}
    \begin{tabular}{CCC}
        \displaystyle\min_{c,r} & r \\
        \text{s.t.} & \norm{c-a^i} \leq r & i=1,\ldots,n
    \end{tabular}
\end{center}
where $c\in\mathbb{R}^d$ and $r\in\mathbb{R}$ are the decision variables corresponding to the center and radius of the ball respectively.

As detailed in \cite{two-algorithms}, the MEB problem can be transformed to a convex quadratic problem (QP) by squaring the constraints and defining a new decision variable $\gamma=r^2$:

\begin{center}
    \begin{tabular}{CLC}
        \displaystyle\min_{c,\gamma} & \gamma \\
        \text{s.t.} & \left(a^i\right)^Ta^i - 2\left(a^i\right)^Tc + c^Tc \leq \gamma & i=1,\ldots,n
    \end{tabular}
\end{center}
\end{definition}
This problem has $d+1$ variables and $n$ constraints, which can make it slow to find an optimal solution by using a solver such as Xpress or Gurobi, but many alternative approaches are present in the literature which offer very fast solutions within a guaranteed accuracy (\cite{core-sets}, \cite{two-algorithms}).



\section{Minimum Enclosing Ball with Outliers}
\subsection{Formulation}
\begin{definition}\label{mebwo}
We may extend Definition \ref{meb} to the Minimum Enclosing Ball with Outliers (MEBwO) problem as follows:
\begin{center}
    \begin{tabular}{CCC}
         \displaystyle\min_{c,r,\xi} & r \\
         \text{s.t.} & \norm{c-a^i} \leq r + M\xi_i & i=1,\ldots,n \\
         & \displaystyle\sum_{i=1}^n\xi_i \leq k \\
         & \xi_i \in \binary & i=1,\ldots,n
    \end{tabular}
\end{center}

where $\xi_i$ are binary variables corresponding to the distance constraint on each variable, with $M\in\mathbb{R}$ a sufficiently large scalar. One can interpret this as, if $\xi_i=1$ for some $i\in\left\{1,\ldots,n\right\}$, then $a_i$ does not need to be inside the ball. The constraint $\sum_{i=1}^n\xi_i\leq k$ where $k=\eta\cdot n$ ensures that $\eta\%$ of the data is covered.

We can instead extend the quadratic program in Definition \ref{meb} to get the following mixed integer quadratic program formulation:
\begin{center}
    \begin{tabular}{CLC}
        \displaystyle\min_{c,\gamma, \xi} & \gamma \\
        \text{s.t.} & \left(a^i\right)^Ta^i - 2\left(a^i\right)^Tc + c^Tc \leq \gamma + M\xi_i & i=1,\ldots,n \\
        & \displaystyle\sum_{i=1}^n\xi_i \leq k \\
        & \xi_i \in \binary & i=1,\ldots,n
    \end{tabular}
\end{center}
\end{definition}
\begin{remark}
One may note that when we relax $\xi_i$ to be continuous variables, i.e. $0\leq\xi_i\leq1$ for $i=1,\ldots,n$, we have again a convex quadratic program since we have then added only continuous variables and linear constraints to the base quadratic program.
\end{remark}

This model gives us a way to solve the MEBwO problem optimally for a given data set. However, by examining the structure of this model we can expect the solution times to be unreasonable for any meaningfully large instance. Note that, of $n$ many binary variables $\xi_i$, we can choose up to $k\leq n$ many of them to have a value of 1, though by Proposition \ref{adding data} we know that adding data gives us an MEB with potentially higher radius, so conversely removing data can potentially lower the radius, meaning we will always pick the highest possible number of points ($k$ many) to be excluded. Thus, by a total brute force search we may expect to explore $\binom{n}{k}$ many individual MEB problems. Modern optimization solvers will work more efficiently than this, utilizing techniques such as branch-and-bound (first proposed by \cite{bnb}). Regardless, we expect a very large solution space which leads to exponentially larger solution times as we will see in Section \ref{exact benchmarks}.


\subsection{On the Big M Parameter}
The ``big $M$ constraint" in Definition \ref{mebwo} is a commonly known technique within Operational Research and an important question is that of what $M$ do we choose? A sufficiently large $M$ is required such that when the corresponding binary variable is equal to $1$, the constraint is effectively nullified. But, as we will see within this section, a value of $M$ which is too large will lead to a less effective model.

\subsubsection{An Upper Bound on $M$}
Finding a suitable value of $M$ depends heavily on the nature of the problem the constraint is being applied to. For the minimum enclosing ball with outliers problem, where we are concerned with the distances $\norm{c-a^i}$ being less than the radius $r$, we look to add a value to $r$ such that for any reasonable $c$, this constraint is always satisfied. Thus, a candidate upper bound for $M$ may be found by calculating each pairwise distance within the data and recording the largest distance. Formally, this may be written as $M:= \max\left\{\norm{a^i-a^j}: i,j = 1,\ldots,n, i\neq j\right\}$.
\subsubsection{Solution Times}
One concern with our choice of $M$ is the effect on the solution time of the solver.

\subsubsection{Quality of $\xi_i$ Solutions in the QP Relaxation}
Another concern regarding the value of $M$ is the effect on the optimal solutions for the relaxed $\xi_i$ variables in the QP relaxation. This is important as good quality relaxed solutions are essential to (TODO: reference to relaxation-based heuristic goes here).


% =============================================================================
% Chapter 3
% =============================================================================

\chapter{Algorithmic Methods}
In this chapter we propose some non-exact construction methods to solving the MEBwO problem which return a feasible solution. We also investigate some improvement heuristics which, given a feasible MEB or MEBwO, seek to locally find a new center such that a smaller radius can be found.

\section{Construction Methods}
\subsection{Relaxation-Based Heuristic}
This heuristic works by first solving the relaxed QP formulation for the MEBwO problem in Definition \ref{mebwo}, and then making the assumption that a higher value of $\xi_i$ (i.e. closer to 1) means that the model treats the data point $a^i$ as ``more" of an outlier. From this assumption, we pick the largest $k$ values of $xi_i$ and treat each corresponding $a^i$ as an outlier, therefore treating the remaining data as inliers. We then solve the MEB problem for this remaining data. A more formal outline of this algorithm is detailed in Algorithm \ref{relaxation heuristic}.

This method will always return a feasible solution that covers $\eta\%$ of the data as we will always remove $k=n(1-\eta)$ many points. As discussed in the previous chapter, solving the final MEB problem is relatively easy and selecting outliers using the MEBwO QP relaxation is trivial, but the difficulty lays in solving the initial QP relaxation, and as such a heuristic method which can solve this model quickly and return approximate solutions to each $xi_i$ is a valuable direction for further research in order to speed up this method.

\begin{algorithm}[H]\label{relaxation heuristic}
    \SetAlgoLined
    \KwIn{Data set $\A=\left\{a^1,\ldots,a^n\right\}$, percentage of data to be covered $\eta\in[0,1]$, error tolerance for MEB heuristic $\epsilon>0$, big $M$ parameter $M>0$}
    \KwOut{Center $c\in\mathbb{R}^d$, radius $r\in\mathbb{R}$}
     $\xi=\left[\xi_1,\ldots,\xi_n\right]$ relaxed binary variables returned by relaxed MEBwO solver for $\A$, $\eta$, and $M$\;
     Let $\xi'$ be the smallest $k=\floor{\eta\cdot n}$ elements of $\xi$\;
     Let $\A':=\left\{a^i\in\A: \xi_i\in\xi',\ i=1,\ldots,n\right\}$\;
     Let $c,r$ be the center and radius of $\MEB(\A')$\;
     
    \caption{Relaxation-Based Heuristic}
\end{algorithm}
\subsection{Peeling-based Heuristic}
\subsection{Shenmaier's Approximation}

\section{Improvement Heuristics}
\subsection{Direction-Constrained Single-Step Heuristic}
\subsection{Direction-Constrained MEB}

% =============================================================================
% Chapter 4
% =============================================================================
\chapter{Implementation and Benchmarks}\label{implementation}
\section{Code Implementation}
\subsection{Data}

\section{Experiments}
\subsection{Methodology}
\subsection{Exact Method}\label{exact benchmarks}
\subsection{Construction Methods}
\subsubsection{Relaxation-based Heuristic}
\subsubsection{Peeling-based Heuristic}
\subsubsection{Shenmaier's Approximation}

\subsection{Improvement Heuristics}
\subsubsection{Direction-Constrained Single-Step Heuristic}
\subsubsection{Direction-Constrained MEB}

\section{Discussion}

% =============================================================================
% Chapter 5
% =============================================================================
\chapter{Conclusion}

% =============================================================================
%the entries have to be in the file literature.bib
\bibliography{literature}
\clearpage

\appendix
\section*{Appendices}
\addcontentsline{toc}{section}{Appendices}

\section{An Appendix}
\label{app:one}

Some stuff.
\clearpage

\section{Another Appendix}
\label{app:two}

Some other stuff.

\end{document}

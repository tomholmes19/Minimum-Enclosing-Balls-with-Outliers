\documentclass[11pt,twoside]{report}
\usepackage{geometry}
\usepackage{enumerate}
\usepackage{latexsym,booktabs}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[singlespacing]{setspace}
% ===== Defaults^
\usepackage{amsthm}
\usepackage{hyperref}           % hyperlinks
\usepackage{array}              % for \newcolumntype macro
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{mathtools}

%TC:ignore
\newcolumntype{C}{>{$}c<{$}}    % math-mode version of "l" column type
\newcolumntype{L}{>{$}l<{$}} 
%TC:endignore

\newcommand{\A}{\mathcal{A}} %% I am lazy
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\binary}{\left\{0,1\right\}}

\DeclareMathOperator{\MEB}{MEB}
\DeclareMathOperator{\MEBwO}{MEBwO}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\theoremstyle{definition}
% ==== Mine^

\geometry{a4paper,left=2cm,right=2.0cm, top=2cm, bottom=2.0cm}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem*{remark}{Remark}
\numberwithin{theorem}{section}
\numberwithin{definition}{section}
\numberwithin{lemma}{section}
\numberwithin{proposition}{section}
\numberwithin{equation}{section}


\begin{document}
\pagestyle{empty}

% =============================================================================
% Title page
% =============================================================================
\begin{titlepage}
\vspace*{.5em}
\center
\textbf{\large{The School of Mathematics}} \\
\vspace*{1em}
\begin{figure}[!h]
\centering
\includegraphics[width=180pt]{CentredLogoCMYK.jpg}
\end{figure}
\vspace{2em}
\textbf{\Huge{Minimum Enclosing Balls with Outliers}}\\[2em]
\textbf{\LARGE{by}}\\
\vspace{2em}
\textbf{\LARGE{Thomas Holmes}}\\
\vspace{6.5em}
\Large{Dissertation Presented for the Degree of\\
MSc in Operational Research with Computational Optimization}\\
\vspace{6.5em}
\Large{August 2021}\\
\vspace{3em}
\Large{Supervised by\\Dr E. Alper Yıldırım}
\vfill
\end{titlepage}

\cleardoublepage

% =============================================================================
% Abstract, acknowledgments, and own work declaration
% =============================================================================
\vspace*{10mm}
\begin{center}
\textbf{\huge{Abstract}}
\end{center}

Here comes your abstract ...

\clearpage
\vspace*{10mm}
\begin{center}
\textbf{\huge{Acknowledgments}}
\end{center}

Here come your acknowledgments ...

\clearpage

\vspace*{10mm}
\begin{center}
\textbf{\huge{Own Work Declaration}}
\end{center}
\vspace*{20mm}

\noindent I declare that this thesis was composed by myself and that the work contained therein is my own, except where explicitly stated otherwise in the text.

\vspace*{10mm}

\begin{flushright}
\textit{(Thomas Holmes)}
\end{flushright}

\cleardoublepage



% =============================================================================
% Table of contents, tables, and pictures (if applicable)
% =============================================================================
\pagestyle{plain}
\setcounter{page}{1}
\pagenumbering{Roman}

\tableofcontents
\clearpage
\listoftables
\listoffigures
\cleardoublepage

\pagenumbering{arabic}
\setcounter{page}{1}

\nocite{*}
\bibliographystyle{abbrv}
\clearpage
% =============================================================================
% Main body
% =============================================================================

% =============================================================================
% Chapter 1
% =============================================================================
\chapter{Introduction}
\section{Motivation}
\section{Outline}

% =============================================================================
% Chapter 2
% =============================================================================
\chapter{Problem Definition and Literature Review}\label{exact}
\section{Preliminaries}
In this paper we shall denote our data set of finite vectors as $\mathcal{A} = \left\{a^1,\ldots,a^n\right\}\subseteq\mathbb{R}^d$ for $n,d\in\mathbb{N}$. The center of a ball is represented by a vector $c\in\mathbb{R}^d$, the radius a scalar $r\in\mathbb{R}$, and the squared radius $\gamma=r^2$. We denote the percentage of inliers for a minimum enclosing ball with outliers by $\eta\in(0,1]$, i.e. if $\eta=0.9$ then we seek a ball which covers $90\%$ of our data.

\begin{definition}
Let $c\in\mathbb{R}^n$ and $r\in\mathbb{R}$. Then the \textit{ball} with center $c$ and radius $r$ is the set
\begin{equation*}
    B(c;r) = \left\{x\in\mathbb{R}^n : \norm{x-c} \leq r\right\}
\end{equation*}
where $\norm{\cdot}:L\to\mathbb{R}$ denotes the standard Euclidean norm on a vector space $L$ (in this paper, $L=\mathbb{R}^n$).
\end{definition}

\begin{definition}
The \textit{minimum enclosing ball} of $\A$, denoted by $\MEB(\A)$, is the ball $B(c^*,r^*)$ where $\A\subseteq B(c^*,r^*)$ and if any $B(c,r)$ exists such that $\mathcal{A}\subseteq B(c,r)$ then $r^*\leq r$.
\end{definition}

\begin{theorem}
For a given set $\A$, $\MEB(\A)$ exists and is unique.
\end{theorem}
\begin{proof}
See \cite[page 5]{two-algorithms}.
\end{proof}

\begin{proposition}\label{adding data}
Let $a'\in\mathbb{R}^n$ where $a'\notin\A$. Suppose $B(c,r)=\MEB(\A)$ and $B(c',r')=\MEB(\A\cap\left\{a'\right\})$. Then $r\leq r'$.
\end{proposition}
\begin{proof}
Clearly we have $\A\subseteq\A\cap\left\{a'\right\}$, then since $\A\cap\left\{a'\right\}\subseteq B(c',r')$ we have $\mathcal{A}\subseteq B(c',r')$ by transitivity. Thus $r\leq r'$ with equality only if $a'\in B(c,r)$ by uniqueness.
\end{proof}

\begin{proposition}\label{removing data}
Suppose $a'\in\A$. Let $B(c,r)=\MEB(\A)$, $B(c',r')=\MEB(\A\setminus\left\{a'\right\})$. Then $r'\leq r$.
\end{proposition}
\begin{proof}
Clearly we have $\A\setminus\left\{a'\right\}\subseteq\A$ then since $\A\setminus\left\{a'\right\}\subseteq B(c',r')$ and $\A\setminus\left\{a'\right\}\subseteq B(c,r)$ we have $r'\leq r$ since $B(c',r')$ is the MEB for $\A\setminus\left\{a'\right\}$, with equality only if $\MEB(\A\setminus\left\{a'\right\} = \MEB(\A)$ by uniqueness.
\end{proof}
\begin{remark}
The prior two propositions tell us that when we add data to a set, the resulting MEB is either unchanged or bigger. Conversely, when we remove data from a set, the resulting MEB is either unchanged or smaller.
\end{remark}

\begin{definition}[{{\cite[page 2]{core-sets}}}]
Let $r^*$ be the radius of $\MEB(\A)$. A ball $B(c;(1+\epsilon)r)$ is a \textit{$(1+\epsilon)$-approximation} of $\MEB(\A)$ if $r\leq r^*$ and $\mathcal{A}\subseteq B(c;(1+\epsilon)r)$.
\end{definition}



\section{Minimum Enclosing Ball}
\begin{definition}\label{meb}
The optimization model formulation of the Minimum Enclosing Ball (MEB) problem is as follows:
\begin{center}
    \begin{tabular}{CCC}
        \displaystyle\min_{c,r} & r \\
        \text{s.t.} & \norm{c-a^i} \leq r & i=1,\ldots,n
    \end{tabular}
\end{center}
where $c\in\mathbb{R}^d$ and $r\in\mathbb{R}$ are the decision variables corresponding to the center and radius of the ball respectively.

As detailed in \cite{two-algorithms}, the MEB problem can be transformed to a convex quadratic problem (QP) by squaring the constraints and defining a new decision variable $\gamma=r^2$:

\begin{center}
    \begin{tabular}{CLC}
        \displaystyle\min_{c,\gamma} & \gamma \\
        \text{s.t.} & \left(a^i\right)^Ta^i - 2\left(a^i\right)^Tc + c^Tc \leq \gamma & i=1,\ldots,n
    \end{tabular}
\end{center}
\end{definition}
This problem has $d+1$ variables and $n$ constraints, which can make it slow to find an optimal solution by using a solver such as Xpress or Gurobi, but many alternative approaches are present in the literature which offer very fast solutions within a guaranteed accuracy (\cite{core-sets}, \cite{two-algorithms}).

We will discuss here one such algorithm to solve the MEB problem from \cite{core-sets} that will be used in algorithms in Chapter \ref{algorithms}. The central idea is that of the \textit{core-set} \cite{agarwal2005geometric}, which is a small set of points that approximate the shape of a larger set of points. For example, a circle can be represented by a core-set of three points which lie on the boundary of the circle.

%TODO: image of 2d circle with core-set here

The idea of the algorithm is to create a candidate core-set, check if the MEB of this core-set contains all of the input data, and if so return the MEB, if not grow the core-set by adding the furthest point from the center of the candidate MEB. For further details please see \cite{core-sets}.

\begin{algorithm}[H]\label{core-set algorithm}
    \SetAlgoLined
    \KwIn{Data set $\A=\left\{a^1,\ldots,a^n\right\}$, error tolerance $\epsilon>0$}
    \KwOut{$(1+\epsilon)$-approximation to $\MEB(\A)$ and an $O(1/\epsilon)$-size core-set}
    
    Let $p$ be any point in $\A$ (can be chosen randomly)\;
    $q=\arg\max_{a\in\A}\norm{p-a}$\;
    $q'=\arg\max_{a\in\A}\norm{q-a}$\;
    $X := \left\{q,q'\right\}$\;
    $\delta := \epsilon^2/163$\;
    \While{True}{
        Let $B(c',r')$ denote the $(1+\delta)$-approximation to $\MEB(X)$ returned by solver\;
        \eIf{$\A\subseteq B(c',(1+\epsilon/2)r'$}{
            break\;
        }{
            $p:=\arg\max_{a\in\A}\norm{a-X}$
        }
        $X := X\cup\left\{a\right\}$
    }
    \KwRet $B(c',(1+\epsilon/2))$, $X$
    \caption{Core-Set Algorithm for the MEB Problem \cite{core-sets}}
\end{algorithm}


\section{Minimum Enclosing Ball with Outliers}
\subsection{Formulation}
\begin{definition}\label{mebwo}
We may extend Definition \ref{meb} to the Minimum Enclosing Ball with Outliers (MEBwO) problem as follows:
\begin{center}
    \begin{tabular}{CCC}
         \displaystyle\min_{c,r,\xi} & r \\
         \text{s.t.} & \norm{c-a^i} \leq r + M\xi_i & i=1,\ldots,n \\
         & \displaystyle\sum_{i=1}^n\xi_i \leq k \\
         & \xi_i \in \binary & i=1,\ldots,n
    \end{tabular}
\end{center}

where $\xi_i$ are binary variables corresponding to the distance constraint on each variable, with $M\in\mathbb{R}$ a sufficiently large scalar. One can interpret this as, if $\xi_i=1$ for some $i\in\left\{1,\ldots,n\right\}$, then $a_i$ does not need to be inside the ball. The constraint $\sum_{i=1}^n\xi_i\leq k$ where $k=\eta\cdot n$ ensures that $\eta\%$ of the data is covered.

We can instead extend the quadratic program in Definition \ref{meb} to get the following mixed integer quadratic program formulation:
\begin{center}
    \begin{tabular}{CLC}
        \displaystyle\min_{c,\gamma, \xi} & \gamma \\
        \text{s.t.} & \left(a^i\right)^Ta^i - 2\left(a^i\right)^Tc + c^Tc \leq \gamma + M\xi_i & i=1,\ldots,n \\
        & \displaystyle\sum_{i=1}^n\xi_i \leq k \\
        & \xi_i \in \binary & i=1,\ldots,n
    \end{tabular}
\end{center}
\end{definition}
\begin{remark}
One may note that when we relax $\xi_i$ to be continuous variables, i.e. $0\leq\xi_i\leq1$ for $i=1,\ldots,n$, we have again a convex quadratic program since we have then added only continuous variables and linear constraints to the base quadratic program.
\end{remark}

This model gives us a way to solve the MEBwO problem optimally for a given data set. However, by examining the structure of this model we can expect the solution times to be unreasonable for any meaningfully large instance. Note that, of $n$ many binary variables $\xi_i$, we can choose up to $k\leq n$ many of them to have a value of 1, though by Proposition \ref{removing data} we know that removing data gives us an MEB with potentially lower radius, meaning we will always pick the highest possible number of points ($k$ many) to be excluded. Thus, by a total brute force search we may expect to explore $\binom{n}{k}$ many individual MEB problems. Modern optimization solvers will work more efficiently than this, utilizing techniques such as branch-and-bound (first proposed by \cite{bnb}). Regardless, we expect a very large solution space which leads to exponentially larger solution times as we will see in Section \ref{exact benchmarks}.


\subsection{On the Big M Parameter}
The ``big $M$ constraint" in Definition \ref{mebwo} is a commonly known technique within Operational Research and an important question is that of what $M$ do we choose? A sufficiently large $M$ is required such that when the corresponding binary variable is equal to $1$, the constraint is effectively nullified. But, as we will see within this section, a value of $M$ which is too large will lead to a less effective model.

\subsubsection{An Upper Bound on $M$}
Finding a suitable value of $M$ depends heavily on the nature of the problem the constraint is being applied to. For the minimum enclosing ball with outliers problem, where we are concerned with the distances $\norm{c-a^i}$ being less than the radius $r$, we look to add a value to $r$ such that for any reasonable $c$, this constraint is always satisfied. Thus, a candidate upper bound for $M$ may be found by calculating each pairwise distance within the data and recording the largest distance. Formally, this may be written as $M:= \max\left\{\norm{a^i-a^j}: i,j = 1,\ldots,n, i\neq j\right\}$.
\subsubsection{Solution Times}
One concern with our choice of $M$ is the effect on the solution time of the solver.

\subsubsection{Quality of $\xi_i$ Solutions in the QP Relaxation}
Another concern regarding the value of $M$ is the effect on the optimal solutions for the relaxed $\xi_i$ variables in the QP relaxation. This is important as good quality relaxed solutions are essential to Algorithm \ref{relaxation heuristic}.

\section{Literature Review}\label{lit review}

% =============================================================================
% Chapter 3
% =============================================================================

\chapter{Heuristic and Approximation Algorithms}\label{algorithms}

In this chapter we propose some non-exact construction methods to solving the MEBwO problem which return a feasible solution. We also investigate some improvement heuristics which, given a feasible MEB or MEBwO, seek to locally find a new center such that a smaller radius can be found.

\section{Construction Methods}
\subsection{Relaxation-Based Heuristic}
This heuristic works by first solving the relaxed QP formulation for the MEBwO problem in Definition \ref{mebwo}, and then making the assumption that a higher value of $\xi_i$ (i.e. closer to 1) means that the model treats the data point $a^i$ as ``more" of an outlier. From this assumption, we pick the largest $k$ values of $xi_i$ and treat each corresponding $a^i$ as an outlier, therefore treating the remaining data as inliers. We then solve the MEB problem for this remaining data. A more formal outline of this algorithm is detailed in Algorithm \ref{relaxation heuristic}.

This method will always return a feasible solution that covers $\eta\%$ of the data as we will always remove $k=n(1-\eta)$ many points. As discussed in the previous chapter, solving the final MEB problem is relatively easy and selecting outliers using the MEBwO QP relaxation is trivial, but the difficulty lays in solving the initial QP relaxation, and as such a heuristic method which can solve this model quickly and return approximate solutions to each $xi_i$ is a valuable direction for further research in order to speed up this method.

\begin{algorithm}[H]\label{relaxation heuristic}
    \SetAlgoLined
    \KwIn{Data set $\A=\left\{a^1,\ldots,a^n\right\}$, percentage of data to be covered $\eta\in[0,1]$, error tolerance for MEB heuristic $\epsilon>0$, big $M$ parameter $M>0$}
    \KwOut{Center $c\in\mathbb{R}^d$, radius $r\in\mathbb{R}$}
     $\xi=\left[\xi_1,\ldots,\xi_n\right]$ relaxed binary variables returned by relaxed MEBwO solver for $\A$, $\eta$, and $M$\;
     Let $\xi'$ be the smallest $k=\floor{\eta\cdot n}$ elements of $\xi$\;
     Let $\A':=\left\{a^i\in\A: \xi_i\in\xi',\ i=1,\ldots,n\right\}$\;
     Let $c,r$ be the center and radius of $\MEB(\A')$ returned by heuristic\;
     \KwRet $B(c, r)$\;
     
    \caption{Relaxation-Based Heuristic}
\end{algorithm}
\subsection{Shrinking Heuristic}
This heuristic works by simply computing an initial MEB $B(c,r)$ using a heuristic such as Algorithm \ref{core-set algorithm}, then computing the $k$ closest points and returning the ball $B(c,r')$ where $r'$ is the maximum distance from $c$ to each of the $k$ closest points. See Algorithm \ref{shrink} for a detailed outline of this algorithm.

\begin{algorithm}[H]\label{shrink}
    \SetAlgoLined
    \KwIn{Data set $\A$, $\eta\in[0,1]$, $\epsilon>0$}
    \KwOut{Center $c\in\mathbb{R}^d$, radius $r\in\mathbb{R}$}
    Let $c$ be the center of the MEB returned by a heuristic for $\MEB(\A)$\;
    $D:=\left\{\norm{c-x}: x\in\A\right\}$\;
    Let $D'$ be $D$ sorted in ascending order\;
    Let $\delta$ be the $(k-1)$th element of $D'$ (indexing from 0)\;
    $\A' = \left\{a^i\in\A: D[i] \leq \delta, i=1,\ldots,n\right\}$\;
    $r = \max_{a\in\A'}\norm{c-a}$\;
    \KwRet $B(c,r)$\;
    
    \caption{Shrinking Heurstic}
\end{algorithm}
\subsection{Shenmaier's Approximation}
This approximation scheme developed by Vladimir Shenmaier in \cite[Algorithm 1]{SHENMAIER201581} is a brute-force algorithm to solving the MEBwO problem with $O(n^2d)$ time complexity. It works by considering each point in the input set, finding the $k$-closest points to that point, then returning the point-distance pair such that the maximum distance from each point to each other point in its $k$-closest points is minimised. See Algorithm \ref{shenmaier} for a detailed outline of this algorithm.
\begin{algorithm}\label{shenmaier}
    \KwIn{Data set $\A$, $\eta\in[0,1]$}
    \KwOut{Center $c\in\mathbb{R}^d$, radius $r\in\mathbb{R}$}
    \For{$i=1,\ldots,n$}{
        $D:=\left\{\norm{c-x}: x\in\A\right\}$\;
        Let $D'$ be $D$ sorted in ascending order\;
        Let $\delta_i$ be the $(k-1)$th element of $D'$ (indexing from 0)\;
    }
    $i^* := \arg\min_{i=1,\ldots,n}$\;
    $c := a^{i^*}$\;
    $r := \delta_{i^*}$\;
    \KwRet{$B(c,r)$}\;
    
    \caption{Shenmaier's Approximation \cite[Algorithm 1]{SHENMAIER201581}}
\end{algorithm}
\section{Improvement Heuristics}
\subsection{Direction-Constrained Single-Step Heuristic}
\subsection{Direction-Constrained MEB}

% =============================================================================
% Chapter 4
% =============================================================================
\chapter{Implementation and Experiments}\label{implementation}
\section{Code Implementation}
\subsection{Data}

\section{Experiments}
\subsection{Methodology}
\subsection{Exact Model}\label{exact benchmarks}
\subsection{Construction Methods}
\subsubsection{Relaxation-based Heuristic}
\subsubsection{Shrinking Heuristic}
\subsubsection{Shenmaier's Approximation}

\subsection{Improvement Heuristics}
\subsubsection{Direction-Constrained Single-Step Heuristic}
\subsubsection{Direction-Constrained MEB}

\section{Discussion}

% =============================================================================
% Chapter 5
% =============================================================================
\chapter{Conclusion}

% =============================================================================
%the entries have to be in the file literature.bib
\bibliography{literature}
\clearpage

\appendix
\section*{Appendices}
\addcontentsline{toc}{section}{Appendices}

\section{An Appendix}
\label{app:one}

Some stuff.
\clearpage

\section{Another Appendix}
\label{app:two}

Some other stuff.

\end{document}

\documentclass[11pt,twoside]{report}
\usepackage{geometry}
\usepackage{enumerate}
\usepackage{latexsym,booktabs}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[singlespacing]{setspace}
% ===== Defaults^
\usepackage{amsthm}
\usepackage{hyperref}           % hyperlinks
\usepackage{array}              % for \newcolumntype macro
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{mathrsfs} %% mathscr for powerset

\usepackage{caption}
\usepackage{subcaption}

%TC:ignore
\newcolumntype{C}{>{$}c<{$}}    % math-mode version of "l" column type
\newcolumntype{L}{>{$}l<{$}} 
%TC:endignore

\newcommand{\A}{\mathcal{A}} %% I am lazy
\newcommand{\K}{\mathcal{K}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % norm
\newcommand{\binary}{\left\{0,1\right\}} % lazy

\newcommand{\datafigure}[4]{
    \begin{figure}
    \centering
    \begin{subfigure}[b]{0.3333\textwidth}
        \centering
        \includegraphics[width=\textwidth]{#1}
    \end{subfigure}
    \begin{subfigure}[b]{0.3333\textwidth}
        \centering
        \includegraphics[width=\textwidth]{#2}
    \end{subfigure}
    \hfill
    \caption{#3}
    \label{#4}
\end{figure}
}


\DeclareMathOperator{\MEB}{MEB}
\DeclareMathOperator{\MEBwO}{MEBwO}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\theoremstyle{definition}

\usetikzlibrary{shapes.misc} % crosses
\tikzset{cross/.style={cross out, draw=black, minimum size=2*(#1-\pgflinewidth), inner sep=0pt, outer sep=0pt},
%default radius will be 1pt. 
cross/.default={2pt}}

\graphicspath{{images/}}
% ==== Mine^

\geometry{a4paper,left=2cm,right=2.0cm, top=2cm, bottom=2.0cm}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem*{remark}{Remark}
\numberwithin{theorem}{section}
\numberwithin{definition}{section}
\numberwithin{lemma}{section}
\numberwithin{proposition}{section}
\numberwithin{equation}{section}
\numberwithin{figure}{section}


\begin{document}
\pagestyle{empty}
\bibliographystyle{abbrv}
% =============================================================================
% Title page
% =============================================================================
\begin{titlepage}
\vspace*{.5em}
\center
\textbf{\large{The School of Mathematics}} \\
\vspace*{1em}
\begin{figure}[!h]
\centering
\includegraphics[width=180pt]{CentredLogoCMYK.jpg}
\end{figure}
\vspace{2em}
\textbf{\Huge{Minimum Enclosing Balls with Outliers}}\\[2em]
\textbf{\LARGE{by}}\\
\vspace{2em}
\textbf{\LARGE{Thomas Holmes}}\\
\vspace{6.5em}
\Large{Dissertation Presented for the Degree of\\
MSc in Operational Research with Computational Optimization}\\
\vspace{6.5em}
\Large{August 2021}\\
\vspace{3em}
\Large{Supervised by\\Dr E. Alper Yıldırım}
\vfill
\end{titlepage}

\cleardoublepage

% =============================================================================
% Abstract, acknowledgments, and own work declaration
% =============================================================================
\vspace*{10mm}
\begin{center}
\textbf{\huge{Abstract}}
\end{center}

Here comes your abstract ...

\clearpage
\vspace*{10mm}
\begin{center}
\textbf{\huge{Acknowledgments}}
\end{center}

Here come your acknowledgments ...

\clearpage

\vspace*{10mm}
\begin{center}
\textbf{\huge{Own Work Declaration}}
\end{center}
\vspace*{20mm}

\noindent I declare that this thesis was composed by myself and that the work contained therein is my own, except where explicitly stated otherwise in the text.

\vspace*{10mm}

\begin{flushright}
\textit{(Thomas Holmes)}
\end{flushright}

\cleardoublepage



% =============================================================================
% Table of contents, tables, and pictures (if applicable)
% =============================================================================
\pagestyle{plain}
\setcounter{page}{1}
\pagenumbering{Roman}

\tableofcontents
\clearpage
\listoftables
\listoffigures
\cleardoublepage

\pagenumbering{arabic}
\setcounter{page}{1}

\nocite{*}
\clearpage
% =============================================================================
% Main body
% =============================================================================

% =============================================================================
% Chapter 1
% =============================================================================
\chapter{Introduction}
\section{Motivation}
\section{Outline}

% =============================================================================
% Chapter 2
% =============================================================================
\chapter{Problem Definition and Literature Review}\label{exact}
\section{Preliminaries}
In this paper we shall denote our data set of finite vectors as $\mathcal{A} = \left\{a^1,\ldots,a^n\right\}\subseteq\mathbb{R}^d$ for $n,d\in\mathbb{N}$. The center of a ball is represented by a vector $c\in\mathbb{R}^d$, the radius a scalar $r\in\mathbb{R}$, and the squared radius $\gamma=r^2$. We denote the percentage of inliers for a minimum enclosing ball with outliers (MEBwO) by $\eta\in(0,1]$, i.e. if $\eta=0.9$ then we seek a ball which covers $90\%$ of our data. While it may be easier for us to think practically of the MEBwO as covering $\eta\%$ of our data, it is easier mathematically to consider a MEBwO which covers $n-k$ out of $n$ data points, where $k=\floor{n(1-\eta)}$ is the number of outliers.

We would now like to make some formal definitions.

\begin{definition}
    Let $c\in\mathbb{R}^n$ and $r\in\mathbb{R}$. Then the \textit{ball} with center $c$ and radius $r$ is the set
    \begin{equation*}
        B(c;r) = \left\{x\in\mathbb{R}^n : \norm{x-c} \leq r\right\}
    \end{equation*}
    where $\norm{\cdot}:L\to\mathbb{R}$ denotes the standard Euclidean norm on a vector space $L$ (in this paper, $L=\mathbb{R}^n$).
\end{definition}

\begin{definition}
    The \textit{minimum enclosing ball} of $\A$, denoted by $\MEB(\A)$, is the ball $B(c^*,r^*)$ where $\A\subseteq B(c^*,r^*)$ and if any $B(c,r)$ exists such that $\mathcal{A}\subseteq B(c,r)$ then $r^*\leq r$.
\end{definition}

\begin{theorem}\label{thm:unique}
    For a given set $\A$, $\MEB(\A)$ exists and is unique.
\end{theorem}
\begin{proof}
    See \cite[page 5]{two-algorithms}.
\end{proof}

Now, we are interested in the idea that adding or removing data from a data set may have an effect on the radius of the MEB of that data set.
\begin{proposition}\label{adding data}
    Let $a'\in\mathbb{R}^n$ where $a'\notin\A$. Suppose $B(c,r)=\MEB(\A)$ and $B(c',r')=\MEB(\A\cap\left\{a'\right\})$. Then $r\leq r'$.
\end{proposition}
\begin{proof}
    Clearly we have $\A\subseteq\A\cap\left\{a'\right\}$, then since $\A\cap\left\{a'\right\}\subseteq B(c',r')$ we have $\mathcal{A}\subseteq B(c',r')$ by transitivity. Thus $r\leq r'$ with equality only if $a'\in B(c,r)$ by uniqueness.
\end{proof}

\begin{proposition}\label{removing data}
    Suppose $a'\in\A$. Let $B(c,r)=\MEB(\A)$, $B(c',r')=\MEB(\A\setminus\left\{a'\right\})$. Then $r'\leq r$.
\end{proposition}
\begin{proof}
    Clearly we have $\A\setminus\left\{a'\right\}\subseteq\A$ then since $\A\setminus\left\{a'\right\}\subseteq B(c',r')$ and $\A\setminus\left\{a'\right\}\subseteq B(c,r)$ we have $r'\leq r$ since $B(c',r')$ is the MEB for $\A\setminus\left\{a'\right\}$, with equality only if $\MEB(\A\setminus\left\{a'\right\} = \MEB(\A)$ by uniqueness.
\end{proof}
These two propositions tell us that when we add data to a set, the resulting MEB is either unchanged or bigger. Conversely, when we remove data from a set, the resulting MEB is either unchanged or smaller.

In the interest of contextualizing Algorithm \ref{core-set algorithm}, we define the $(1+\epsilon)$-approximation to an MEB and core-sets.
\begin{definition}[{{\cite[page 2]{core-sets}}}]
    Let $\epsilon>0$. Let $r^*$ be the radius of $\MEB(\A)$. A ball $B(c;(1+\epsilon)r)$ is a \textit{$(1+\epsilon)$-approximation} of $\MEB(\A)$ if $r\leq r^*$ and $\mathcal{A}\subseteq B(c,(1+\epsilon)r)$.
\end{definition}

\begin{definition}[{{\cite[page 2]{core-sets}}}]
    Let $\epsilon>0$. A subset $X\subseteq\A$ is a \textit{$\epsilon$-core-set} (typically just referred to as a \textit{core-set}) of $\A$ if $\A\subset B(c,(1+\epsilon)r)$.
\end{definition}
Finally, the following two definitions allow us to formally define the MEBwO.
\begin{definition}
    For a set $\A$, the set of $k$-subsets of $\A$ is the set $\mathfrak{K}=\left\{\K\in\mathscr{P}(\A): |\K|=k\right\}$ where $\mathcal{P}:\textbf{Set}\to\textbf{Set}$ is the usual power set functor.
\end{definition}

\begin{definition}
    For some $\eta\in[0,1]$, let $k=\floor{n(1-\eta)}$ be the number of outliers in $\A$. Then the \textit{minimum enclosing ball with outliers} of $\A$, denoted by $\MEBwO(\A,\eta)$, which covers $\eta\%$ of $\A$, is the ball $B(c^*,r^*)=\MEB(\K^*)$ for $\K^*\in\mathfrak{K}$ a $k$-subset of $\A$ where for all other $k$-subsets $\K\in\mathfrak{K}$, $\K\neq\K^*$, if $B(c,r)=\MEB(\K)$ then $r^*\leq r$.
\end{definition}

We may deduce that for a given data set $\A$, an MEBwO exists since an MEBwO is simply the MEB of some $k$-subset of $\A$, which by Theorem \ref{thm:unique} exists and is unique for that $k$-subset. However, MEBwO's are not unique, as we can have two distinct $k$-subsets of $\A$ which have MEB's of the same radius.

For a simple example, consider the data set $\A=\left\{(-2,-1),(2,-1),(2,1),(-2,1)\right\}$ with $\eta=\frac{1}{2}$. Then $B((-2,0),1)$ and $B((2,0),1)$ are both MEBwO's for $\A$, and MEB's for the $2$-subsets $\left\{(-2,-1), (-2,1)\right\}$, $\left\{(2,-1), (2,1)\right\}$ of $\A$ respectively. See Figure \ref{fig:mebwo non-unique} for a visualisation of this example.

\begin{figure}
    \centering
    \begin{tikzpicture}
        % axes
        \draw[<->] (0, -2.5) -- (0,2.5);
        \draw[<->] (-3.5, 0) -- (3.5, 0);
        
        % points
        \foreach \coord in {(-2,-1), (2,-1)}{
            \draw[fill] \coord circle (1.5pt) node[below]{\coord};
        }
        \foreach \coord in {(2,1), (-2,1)}{
            \draw[fill] \coord circle (1.5pt) node[above]{\coord};
        }
        
        % balls
        \foreach \coord in {(-2,0), (2,0)}{
            \draw \coord circle (1);
            \draw \coord node[cross, red]{};
        }
        
    \end{tikzpicture}
    \caption{Example of MEBwO non-uniqueness}
    \label{fig:mebwo non-unique}
\end{figure}

\section{Minimum Enclosing Ball}
\begin{definition}\label{meb}
    The optimization model formulation of the Minimum Enclosing Ball (MEB) problem is as follows:
    \begin{center}
        \begin{tabular}{CCC}
            \displaystyle\min_{c,r} & r \\
            \text{s.t.} & \norm{c-a^i} \leq r & i=1,\ldots,n
        \end{tabular}
    \end{center}
    where $c\in\mathbb{R}^d$ and $r\in\mathbb{R}$ are the decision variables corresponding to the center and radius of the ball respectively.
    
    We can formulate the MEB problem as a second-order cone program (SOCP) by considering the constraints as second-order cone constraints, i.e.
    \begin{equation*}
        C_i = \left\{(r, c-a^i)\in\mathbb{R}^{d+1}: \norm{c-a^i}\leq r\right\}
    \end{equation*}
    
    As detailed in \cite{two-algorithms}, the MEB problem can be transformed to a convex quadratically constrained program (QCQP) by squaring the constraints and defining a new decision variable $\gamma=r^2$:
    
    \begin{center}
        \begin{tabular}{CLC}
            \displaystyle\min_{c,\gamma} & \gamma \\
            \text{s.t.} & \left(a^i\right)^Ta^i - 2\left(a^i\right)^Tc + c^Tc \leq \gamma & i=1,\ldots,n
        \end{tabular}
    \end{center}
\end{definition}
Since this problem can be modelled as a SOCP we know it may be solved by interior-point methods \cite{socp_ipm}. This problem has $d+1$ variables and $n$ constraints, so it quickly become slow to find an optimal solution for large $n$ and $d$ when using a solver such as Xpress or Gurobi, but many alternative approaches are present in the literature which offer very fast solutions within a guaranteed accuracy (\cite{core-sets}, \cite{two-algorithms}).

We will discuss here one such algorithm to solve the MEB problem from \cite{core-sets} that will be used in algorithms in Chapter \ref{algorithms}. The central idea is that of the \textit{core-set} \cite{badoiu}, which is a small set of points that approximate the shape of a larger set of points. For example, a circle in two dimensions can be represented by a core-set of three points which lie on the boundary of the circle (see Figure \ref{fig:core set example}).

The idea of the algorithm is to create a candidate core-set, check if the MEB of this core-set contains all of the input data, and if so return the MEB, if not grow the core-set by adding the furthest point from the center of the candidate MEB. For further details please see \cite{core-sets}.

\begin{algorithm}[H]\label{core-set algorithm}
    \SetAlgoLined
    \KwIn{Data set $\A=\left\{a^1,\ldots,a^n\right\}$, error tolerance $\epsilon>0$}
    \KwOut{$(1+\epsilon)$-approximation to $\MEB(\A)$ and an $O(1/\epsilon)$-size core-set}
    
    Let $p$ be any point in $\A$ (can be chosen randomly)\;
    $q=\arg\max_{a\in\A}\norm{p-a}$\;
    $q'=\arg\max_{a\in\A}\norm{q-a}$\;
    $X := \left\{q,q'\right\}$\;
    $\delta := \epsilon^2/163$\;
    \While{True}{
        Let $B(c',r')$ denote the $(1+\delta)$-approximation to $\MEB(X)$ returned by solver\;
        \eIf{$\A\subseteq B(c',(1+\epsilon/2)r'$}{
            break\;
        }{
            $p:=\arg\max_{a\in\A}\norm{a-X}$
        }
        $X := X\cup\left\{a\right\}$
    }
    \KwRet $B(c',(1+\epsilon/2))$, $X$
    \caption{Core-Set Algorithm for the MEB Problem \cite{core-sets}}
\end{algorithm}

Algorithm \ref{core-set algorithm} runs in $O\left(\frac{nd}{\epsilon}+\frac{d^2}{\epsilon^{3/2}}\left(\frac{1}{\epsilon}+d\right)\log\frac{1}{\epsilon}\right)$ time \cite[Page 6]{core-sets}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{core_set_example.png}
    \caption{Example of a Core-Set for an MEB}
    \label{fig:core set example}
\end{figure}

\section{Minimum Enclosing Ball with Outliers}
\subsection{Formulation}
\begin{definition}\label{mebwo}
    We may extend Definition \ref{meb} to the Minimum Enclosing Ball with Outliers (MEBwO) problem as follows:
    \begin{center}
        \begin{tabular}{CCC}
             \displaystyle\min_{c,r,\xi} & r \\
             \text{s.t.} & \norm{c-a^i} \leq r + M\xi_i & i=1,\ldots,n \\
             & \displaystyle\sum_{i=1}^n\xi_i \leq k \\
             & \xi_i \in \binary & i=1,\ldots,n
        \end{tabular}
    \end{center}
    
    where $\xi_i$ are binary variables corresponding to the distance constraint on each variable, with $M\in\mathbb{R}$ a sufficiently large scalar. One can interpret this as, if $\xi_i=1$ for some $i\in\left\{1,\ldots,n\right\}$, then $a_i$ does not need to be inside the ball. The constraint $\sum_{i=1}^n\xi_i\leq k$ where $k=\floor{n(1-\eta)}$ ensures that $\eta\%$ of the data is covered.
    
    We can instead extend the QCQP in Definition \ref{meb} to get the following mixed integer QCQP (MIQCQP) formulation:
    \begin{center}
        \begin{tabular}{CLC}
            \displaystyle\min_{c,\gamma, \xi} & \gamma \\
            \text{s.t.} & \left(a^i\right)^Ta^i - 2\left(a^i\right)^Tc + c^Tc \leq \gamma + M\xi_i & i=1,\ldots,n \\
            & \displaystyle\sum_{i=1}^n\xi_i \leq k \\
            & \xi_i \in \binary & i=1,\ldots,n
        \end{tabular}
    \end{center}
\end{definition}
\begin{remark}
    One may note that when we relax $\xi_i$ to be continuous variables, i.e. $0\leq\xi_i\leq1$ for $i=1,\ldots,n$, we have again a convex QCQP since we have then added only continuous variables and linear constraints to the base QCQP.
\end{remark}

This model gives us a way to solve the MEBwO problem optimally for a given data set. However, by examining the structure of this model we can expect the solution times to be unreasonable for any meaningfully large instance. Note that, of $n$ many binary variables $\xi_i$, we can choose up to $k\leq n$ many of them to have a value of 1, though by Proposition \ref{removing data} we know that removing data gives us an MEB with potentially lower radius, meaning we will always pick the highest possible number of points ($k$ many) to be excluded. Thus, by a total brute force search we may expect to explore $\binom{n}{k}$ many individual MEB problems. Modern optimization solvers will work more efficiently than this, utilizing techniques such as branch-and-bound. Regardless, we expect a very large solution space which leads to exponentially larger solution times as we will see in Section \ref{exact benchmarks}.


\subsection{On the Big M Parameter}
The ``big $M$ constraint" in Definition \ref{mebwo} is a commonly known technique within Operational Research and an important question is that of what $M$ do we choose? A sufficiently large $M$ is required such that when the corresponding binary variable is equal to $1$, the constraint is effectively nullified. But, as we will see within this section, a value of $M$ which is too large will lead to a longer solution time with worse solutions.

\subsubsection{An Upper Bound on $M$}
Finding a suitable value of $M$ depends heavily on the nature of the problem the constraint is being applied to. For the minimum enclosing ball with outliers problem, where we are concerned with the distances $\norm{c-a^i}$ being less than the radius $r$, we look to add a value to $r$ such that for any reasonable $c$, this constraint is always satisfied. Thus, a candidate upper bound for $M$ may be found by calculating each pairwise distance within the data and recording the largest distance. Formally, this may be written as $M:= \max\left\{\norm{a^i-a^j}: i,j = 1,\ldots,n, i\neq j\right\}$.

An immediately apparent issue with this approach is that computing $M$ in this way will run in $O(n^2d)$ time. For smaller data sets this is manageable, but for any significantly sized data sets the time taken to compute $M$ is often longer than the time taken to solve the MEBwO problem on the same data.

We can look to \cite[Lemma 1]{core-sets} for a $1/\sqrt{3}$-approximation to the diameter of $\A$ which runs in $O(nd)$ time, by picking a random point $p\in\A$, finding $q$ the furthest point in $\A$ from $p$, finding $q'$ the furthest point in $\A$ from $q$, then setting $D:=\norm{q-q'}$ which is our $1/\sqrt{3}$-approximation to the diameter of $\A$. For a candidate $M$ value we can simply set $M:=\sqrt{3}D$.

\begin{algorithm}[H]\label{alg:diameter approx}
    \KwIn{Data set $\A$, point $p\in\A$}
    \KwOut{Approximate diameter $D\in\mathbb{R}$}
    $q:=\arg\max_{a\in\A}\norm{p-a}$\;
    $q':=\arg\max_{a\in\A}\norm{q-a}$\;
    $D:=\norm{q-q'}$\;
    \KwRet{D}\;
    
    \caption{Diameter Approximation Algorithm \cite[Lemma 1]{core-sets}}
\end{algorithm}
A caveat to this approach is that it the value of $M$ will change depending on the initial random point chosen, and can vary significantly. To demonstrate this, we generate a standard normal data set $\A$ with $n=1000$, $d=10$ (see Section \ref{data}), and by running Algorithm \ref{alg:diameter approx} for each $a\in\A$ we construct a distribution of approximate diameters. From \texttt{src/diameter\_approx\_analysis.py} we find an average diameter of $8.939$ with variance $0.170$ and a density plot is shown in Figure \ref{fig:diameter approx density}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{pw_M_density.png}
    \caption{Density Plot of Approximate Diameters from Algorithm \ref{alg:diameter approx}}
    \label{fig:diameter approx density}
\end{figure}

For more robust benchmarking, we would prefer a method which always returns the same $M$ value for a given data set, so this method is unsuitable for our purposes. This is because, as seen in the previous section, a higher value of $M$ has a negative effect on the run time of the solver so we would prefer to minimize the effect of randomness on our benchmarks.

Our chosen method, which gives reliable estimates for $M$ in a reasonable time is to simply solve the $MEB$ problem for $\A$ and set $M:=2r$ for $B(c,r)=\MEB(\A)$. We know from Theorem \ref{thm:unique} that an MEB always exists and is unique for a given data set, so can always expect to receive the same $M$ value from this method. Using a heuristic approach such as Algorithm \ref{core-set algorithm} gives us a $(1+\epsilon)$-approximation to $\MEB(\A)$, so we will receive an $M$ value that is slightly larger than needed, but the effect on run time is insignificant and given that the returned $M$ value is consistent we do not introduce unneeded randomness into our methodology that is not already inherently present due to using randomly generated data.

\subsubsection{Solution Times} %TODO
One concern with our choice of $M$ is the effect on the solution time of the solver.

\subsubsection{Quality of $\xi_i$ Solutions in the QP Relaxation}
Another concern regarding the value of $M$ for the exat model is the effect on the optimal solutions for the relaxed $\xi_i$ variables in the QP relaxation. This is important as good quality relaxed solutions are essential to Algorithm \ref{relaxation heuristic}, which uses high-valued $\xi_i$ as a proxy for how far a data point lies from the optimal center, and so for how much of an outlier that point is.

We can evaluate the quality and diversity of relaxed solutions by solving the relaxed model on the same data set with different values of $M$. We generate random data sets (see Section \ref{data}) with $n=1000$, $d=10$, and the standard parameters described in Section \ref{data}, then calculate a lower bound for $M^*$ using Algorithm \ref{alg:diameter approx} for each data set, then solve the model for a sequence of multiples of $M^*$ and record the variance of the list of relaxed solutions. We choose to test 50 different values of $M$ in the sequence $\langle M^*(1+k/2)\rangle _{k=1,\ldots,50}$. The implementation can be found in \texttt{src/xi\_analysis.py} and a plot of the variances can be seen in Figure \ref{fig:xi analysis}.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{xi_analysis.png}
    \caption{Variance of Relaxed $\xi_i$ Variables for a Sequence of $M$ Values.}
    \label{fig:xi analysis}
\end{figure}

We can see in Figure \ref{fig:xi analysis} that, for each data set, the variance of the relaxed solutions for $xi_i$ decreases sharply as $M$ increases and tends to zero. Interestingly there is actually a slight increase in variance at first for the normal data, but like the rest this does still tend to zero.

\section{Literature Review}\label{lit review}
We will now conduct a review of the current literature on the MEBwO problem.

Early approaches were developed specifically for the $d=2$ case of finding the minimum enclosing circle with outliers. One such paper is \cite{AGGARWAL199138} which discusses computing Voronoi cells and then performing a search of the resulting Voronoi cells. \cite{efrat} makes use of a parametric search technique proposed by \cite{megiddo}, and \cite{MATOUSEK1995217} uses a randomized search algorithm. These papers all gave exact solutions to the MEBwO problem in the two-dimensional case, while \cite{harpeled} presents some approximate algorithms.

Some more recent approaches are as follows. \cite{SHENMAIER201581} proves the NP-hardness of the MEBwO problem in Euclidean space (Theorem 1) and also that no fully polynomial time approximation scheme exists for the MEBwO problem unless $P=NP$ (Theorem 3). Shenmaier's paper also proposes two algorithms for solving the MEBwO problem in $\mathbb{R}^d$ which both have an approximation guarantee. We implement and benchmark Algorithm 1 in this paper which Shenmaier proves to give 2-approximate solutions, meaning we may get a lower bound on the optimal value of the MEBwO problem for a given data set. This algorithm works by computing the $k$-nearest points to each point in the data set and returning the smallest ball found. The second algorithm, not implemented in this paper, is an extension of the prior algorithm where a grid of candidate centres is constructed in a local neighbourhood of each point. This algorithm provides a tighter approximation guarantee but has significantly worse time complexity. Shenmaier also discusses the dual of the MEBwO problem, and the hardness of this dual is an interesting area of further research.

\cite{cavaleiro} presents an algorithm built on branch-and-bound \cite{bnb} which orders the nodes of the tree in such a way that along with a first-in-first-out search strategy results in a small number of nodes explored. \cite{huding} also presents a branch-and-bound algorithm, this time using the idea of core-sets and a bi-criteria approximation with respect to both the radius and percentage of points covered. This paper also utilizes the algorithm as a classification model on the MNIST \cite{lecun2010mnist} data set (for further details please see Section \ref{data}).

Due to the time constraints of this project and the difficulty of implementing branch-and-bound methods we will not be implementing these branch-and-bound algorithms in this paper, and instead focus on methods which work from a more geometric mindset. We will implement Shenmaier's approximation and also investigate some improvement heuristics which may be run quickly in order to improve the initial solution provided from this method.








% =============================================================================
% Chapter 3
% =============================================================================

\chapter{Heuristic and Approximation Algorithms}\label{algorithms}

In this chapter we propose some non-exact construction methods to solving the MEBwO problem which return a feasible solution. We also investigate some improvement heuristics which, given a feasible MEB or MEBwO, seek to locally find a new center such that a smaller radius can be found.

\section{Construction Methods}
\subsection{Relaxation-Based Heuristic}
This heuristic works by first solving the relaxed QP formulation for the MEBwO problem in Definition \ref{mebwo}, and then making the assumption that a higher value of $\xi_i$ (i.e. closer to 1) means that the model treats the data point $a^i$ as ``more" of an outlier. From this assumption, we pick the largest $k$ values of $xi_i$ and treat each corresponding $a^i$ as an outlier, therefore treating the remaining data as inliers. We then solve the MEB problem for this remaining data. A more formal outline of this algorithm is detailed in Algorithm \ref{relaxation heuristic}.

This method will always return a feasible solution that covers $\eta\%$ of the data as we will always remove $k=\floor{n(1-\eta)}$ many points. As discussed in the previous chapter, solving the final MEB problem is relatively easy and selecting outliers using the MEBwO QP relaxation is trivial, but the difficulty lays in solving the initial QP relaxation, and as such a heuristic method which can solve this model quickly and return approximate solutions to each $xi_i$ is a valuable direction for further research in order to speed up this method.

\begin{algorithm}[H]\label{relaxation heuristic}
    \SetAlgoLined
    \KwIn{Data set $\A=\left\{a^1,\ldots,a^n\right\}$, percentage of data to be covered $\eta\in[0,1]$, error tolerance for MEB heuristic $\epsilon>0$, big $M$ parameter $M>0$}
    \KwOut{Ball $B(c,r)$}
     $\xi=\left[\xi_1,\ldots,\xi_n\right]$ relaxed binary variables returned by relaxed MEBwO solver for $\A$, $\eta$, and $M$\;
     Let $\xi'$ be the smallest $k=\floor{\eta\cdot n}$ elements of $\xi$\;
     Let $\A':=\left\{a^i\in\A: \xi_i\in\xi',\ i=1,\ldots,n\right\}$\;
     Let $c,r$ be the center and radius of $\MEB(\A')$ returned by heuristic\;
     \KwRet $B(c, r)$\;
     
    \caption{Relaxation-Based Heuristic}
\end{algorithm}

\subsection{Shenmaier's Approximation}
\subsubsection{Statement}
This approximation scheme developed by Vladimir Shenmaier in \cite[Algorithm 1]{SHENMAIER201581} is a brute-force algorithm to solving the MEBwO problem with $O(n^2d)$ time complexity. It works by considering each point in the input set, finding the $k$-closest points to that point, then returning the point-distance pair such that the maximum distance from each point to each other point in its $k$-closest points is minimised. See Algorithm \ref{shenmaier} for a detailed outline of this algorithm.
\begin{algorithm}
    \KwIn{Data set $\A$, $\eta\in[0,1]$}
    \KwOut{Ball $B(c,r)$}
    $k=\floor{n(1-\eta)}$\;
    \For{$i=1,\ldots,n$}{
        $D:=\left\{\norm{c-a}: a\in\A\right\}$\;
        Let $D'$ be $D$ sorted in ascending order\;
        Let $\delta_i$ be the $(k-1)$th element of $D'$ (indexing from 0)\;
    }
    $i^* := \arg\min_{i=1,\ldots,n}\delta_i$\;
    $c := a^{i^*}$\;
    $r := \delta_{i^*}$\;
    \KwRet{$B(c,r)$}\;
    
    \caption{Shenmaier's Approximation \cite[Algorithm 1]{SHENMAIER201581}}
    \label{shenmaier}
\end{algorithm}
\subsubsection{Approximation Guarantee}
 From \cite[Theorem 4]{SHENMAIER201581} we know that Algorithm \ref{shenmaier} returns a 2-approximation solution to the MEBwO problem. What this means is that if we have a minimum radius of $r$ returned by Algorithm \ref{shenmaier} for a data set $\A$, then the true optimal value to the MEBwO problem on that data is no less than $r/2$. This is useful for assessing how close a heuristic is to an optimal solution in the worst case when we have a solution via Algorithm \ref{shenmaier}, but not an optimal solution via solving the exact model.

\subsubsection{Complexity}
Referring to Algorithm \ref{shenmaier}, on line 1 we have an assignment which runs in $O(1)$ time. On lines 2 through 5 we have a \texttt{for} loop which runs $n$ many times, and on line 3 we compute a list of $n$ many distances, which for $d$-dimensional vectors runs in $O(nd)$ time. Line 4 involves sorting the aforementioned list, so the time complexity depends on the sorting algorithm used. It is well known within computer science that algorithms are often subject to a time-memory trade off, meaning that it is possible to design algorithms that run faster as a result of using less memory, and vice-versa. We can then assume that a sensible implementation of Algorithm \ref{shenmaier} would use the fastest available sorting algorithm given that the size of our data sets are small enough that we do not run into memory constraints, so the recommended sorting algorithm is Quicksort \cite{hoare1962quicksort} which runs on average in $O(n\log n)$ time. Line 5 simply accesses the $(k-1)$th element of the sorted list and so runs in $O(1)$ time.

Line 6 finds the minimum element from a set of values and runs in $O(n)$ time. Lines 7 and 8 are simple assignments and run in $O(1)$ time. Finally, our overall time complexity is then $O(n^2(d+\log n))$. This will usually reduce to $O(n^2d)$ as typically $d>\log n$, though for particularly large $n$ and very small $d$ the alternative reduction of $O(n^2\log n)$ is of course possible.

\subsection{MEB Shrinking Heuristic}
\subsubsection{Statement}
The MEB Shrinking Heuristic works by computing $\MEB(\A) = B(c,r)$, then simply shrinking the radius until $\eta\%$ of points are covered. This is done by calculating the $n-k$ closest points to $c$ and returning the ball $B(c,r')$ where $r'$ is the greatest distance from $c$ to each of the $n-k$ points.

\begin{algorithm}[H]\label{meb shrink}
    \SetAlgoLined
    \KwIn{Data set $\A$, $\eta\in[0,1]$, $\epsilon>0$}
    \KwOut{Ball $B(c,r)$}
    $k:=\floor{n(1-\eta)}$\;
    Let $c$ be the center of the MEB returned by a heuristic for $\MEB(\A)$\;
    $D:=\left\{\norm{c-x}: x\in\A\right\}$\;
    Let $D'$ be $D$ sorted in ascending order\;
    Let $\delta$ be the $(k-1)$th element of $D'$ (indexing from 0)\;
    $\A' = \left\{a^i\in\A: D[i] \leq \delta, i=1,\ldots,n\right\}$\;
    $r = \max_{a\in\A'}\norm{c-a}$\;
    \KwRet $B(c,r)$\;
    
    \caption{MEB Shrinking Heuristic}
\end{algorithm}

\subsubsection{Complexity}
The complexity of this heuristic mostly depends on the choice of heuristic for solving the MEB problem on $\A$. The dominant term in lines 3 though 7 is on average $O(n\log n)$ from sorting the list of distances, so the overall complexity is the greater out of this and the complexity of the heuristic. Our implementation uses Algorithm \ref{core-set algorithm}, which in this case is $O\left(\frac{nd}{\epsilon}+\frac{d^2}{\epsilon^{3/2}}\left(\frac{1}{\epsilon}+d\right)\log\frac{1}{\epsilon}\right)$ \cite[Page 6]{core-sets}.
\subsection{Average Point Heuristic}
\subsubsection{Statement}
The Average Point Heuristic works by finding the average point, i.e. $c=\frac{1}{n}\sum_{i=1}^n a^i$, then finding the $n-k$ closest points and returning the ball $B(c,r)$ where $r$ is the greatest distance from $c$ to each of the $n-k$ points.

\begin{algorithm}[H]\label{avg point shrink}
    \SetAlgoLined
    \KwIn{Data set $\A$, $\eta\in[0,1]$}
    \KwOut{Ball $B(c,r)$}
    $k:=\floor{n(1-\eta)}$\;
    $c:=\frac{1}{n}\sum_{i=1}^na^i$\;
    $D:=\left\{\norm{c-x}: x\in\A\right\}$\;
    Let $D'$ be $D$ sorted in ascending order\;
    Let $\delta$ be the $(k-1)$th element of $D'$ (indexing from 0)\;
    $r = \max_{a\in\A'}\norm{c-a}$\;
    \KwRet{$B(c,r)$}
    
    \caption{Average Point Shrinking Heuristic}
\end{algorithm}

\subsubsection{Complexity}
This algorithm runs extremely quickly, with the dominating term in the time complexity being $O(n\log n)$ on average as a result of sorting the list of distances on line 4. This is far faster than any other construction method and is more resistant to outliers than the MEB shrinking heuristic.



\section{Improvement Heuristics}
We now consider some improvement heuristics. The two heuristics described here are, strictly speaking, designed to improve an existing MEB rather than an MEBwO. However, quick and effective methods exist for solving the MEB problem and so improvement heuristics have not been needed, while heuristics which solve the MEBwO problem may return a solution which has considerable space for improvement.

The two improvement heuristics work in a similar way, by considering an improving direction in which moving the centre of the MEB in that direction while keeping the same radius retains a feasible solution, then shrinking the radius to the furthest point from the new centre. The Direction-Constrained Single Step Heuristic (DCSSH) works by pure geometric reasoning to find a new centre, while the Direction-Constrained MEB (DCMEB) uses an optimization solver such as Gurobi to find an optimal step in the improving direction to minimize the radius of the new ball.

The inspiration for these heuristics comes from Shenmaier's Approximation, which returns a ball with some centre $c\in\A$. Our assumption is that a better solution exists in a local neighbourhood around $c$, i.e. we find a new centre that does not have to lie on a point in $\A$. Both heuristics consider only a single step, but as they are quick to run we may run each heuristic in succession to build a sequence of steps that lead to an improved solution.

\subsection{Direction-Constrained Single Step Heuristic}
\subsubsection{Derivation}
For a visual aid to this derivation please refer to Figure \ref{vis aid derivation}. Consider $B(\hat{c}, r) = \MEB(\A)$. To find an improving direction we choose the furthest point from $c$, $\hat{a}=\arg\max_{a\in\A}\norm{c-a}$, then the (negative) improving direction is defined to be $\beta=\hat{c}-\hat{a}$. We choose the direction $\hat{a}\to\hat{c}$ as opposed to $\hat{c}\to\hat{a}$ in order to simplify later derivations.

Now that we have a direction $\beta$, we can move the initial ball in this direction by simply subtracting $\beta x$ from $\hat{c}$ where $x\in\mathbb{R}^{\geq0}$ is some scalar, i.e. our new centre is $c=\hat{c}-\beta x$. We would like to choose $x$ such that the new ball remains feasible by ensuring that $\norm{c-a^i}\leq r$ for $i=1,\ldots,n$, and we can do this by considering the distance from each $a^i$ in the direction of $\beta$ to the surface of the initial ball and choose $x$ such that each of these distances is greater than $\norm{\beta x}$.

More formally, for each $a^i\in\A$ let $x_i$ be the scalar such that $a^i+\beta x_i$ lies on the surface of the initial ball, i.e. $\norm{(a^i+\beta x^i)-\hat{c}}=r$. Again, to simplify derivations, denote the direction $\hat{c}\to a^i$ by $\alpha_i=a^i-\hat{c}$. To calculate each $x_i$, we square this equation and set $\gamma=r^2$ to get
\begin{align*}
    \alpha_i^T\alpha_i + 2x\alpha_i^T\beta + x^2\beta^T\beta &= \gamma \\
    \implies \beta^T\beta x^2 + 2\alpha_i^T\beta x + \alpha_i^T\alpha_i - \gamma &= 0.
\end{align*}
Using the quadratic formula we then have
\begin{align*}
    x &= \frac{-2\alpha_i^T\beta \pm \sqrt{4\left(\alpha_i^T\beta\right)^2 - 4\beta^T \beta\left(\alpha_i^T\alpha_i-\gamma\right)}}{2\beta^T\beta} \\[4pt]
    &= \frac{-\alpha_i^T\beta \pm \sqrt{\left(\alpha_i^T\beta\right)^2 - \beta^T \beta\left(\alpha_i^T\alpha_i-\gamma\right)}}{\beta^T\beta}.
\end{align*}
The two solutions to $x$ corresponding to the positive and negative discriminants represent the steps in the positive and negative $\beta$ directions respectively, then since from our derivations we are only interested in the positive $\beta$ direction we can disregard the negative solution for $x$, so we can define the function $Q:\mathbb{R}^d\times\mathbb{R}^d\times\mathbb{R}\to\mathbb{R}$ by
\begin{equation*}
    Q(\alpha_i,\beta,\gamma) = \frac{-\alpha_i^T\beta + \sqrt{\left(\alpha_i^T\beta\right)^2 - \beta^T \beta\left(\alpha_i^T\alpha_i-\gamma\right)}}{\beta^T\beta}.
\end{equation*}


Then let $x=\min\left\{Q(\alpha_i, \beta, \gamma):i=1,\ldots,n\right\}$, so $\norm{\beta x}$ is the smallest distance from each $a^i$ to the surface of the ball in the direction of $\beta$. Finally, by moving by some scalar multiple of $-\beta x$ from $\hat{c}$, we get a new feasible centre $c=\hat{c}-\beta x/s$ for some scalar $s\geq1$. We give a simple proof that this new centre with unchanged radius indeed yields a feasible solution and a visual aid is provided in Figure \ref{vis aid feasibility}. See Algorithm \ref{fig:dcssh} for a detailed outline of this algorithm.

\begin{figure}
    \centering
    \begin{tikzpicture}
        % lines
        \draw[->, color=magenta] (-3,0) -- (-0.05,0) node[midway, below]{$\beta$};
        \draw[->, color=cyan] (0,0) -- (59/60,59/40) node[midway, below right]{$\alpha_i$};
        \draw[dashed] (1,1.5) -- (2.598, 1.5) node[midway, below]{$\norm{\beta}{x}$};
        
        % points
        \draw[fill] (0,0) circle (1.5pt) node[below]{$\hat{c}$};
        \draw[fill] (-3,0) circle (1.5pt) node[left]{$\hat{a}$};
        \draw[fill] (1, 1.5) circle (1.5pt) node[above left]{$a^i$};
        \draw[fill] (2.598, 1.5) circle (1.5pt) node[above right]{$a^i + \beta x$};
        \draw[fill] (-0.75,0) circle (1.5pt) node[above]{$\hat{c}-\frac{\beta x}{s}$};

        % ball
        \draw (0,0) circle (3);
    \end{tikzpicture}
    \caption{Visual Aid for Direction-Constrained Single Step Heuristic}
    \label{vis aid derivation}
\end{figure}

\begin{proposition}\label{imp heuristic feasibility}
Consider the MEB problem for a data set $\A$ denoted by $(P)$. Let $B(\hat{c},r)$ be a feasible solution to $(P)$. Then the ball $B(\hat{c}-\beta x/s, r)$ with $\beta$, $x$, and $s$ as described above, is also a feasible solution to $(P)$.
\end{proposition}
\begin{proof}
For each $a^i\in\A$ let $\hat{a}^i_\beta$ be the point on the surface of $B(\hat{c},r)$ in the direction of $\beta$ from $a^i$. Let $a^i_\beta=\hat{a^i}_\beta - \beta x/s$ be the point on the surface of $B(\hat{c}-\beta x/s, r)$ in the direction of $\beta$ from $a^i$. Let $c=\hat{c}-\beta x/s$. Then, by the triangle inequality, we have
\begin{align*}
    \norm{c-a^i_\beta} &\geq \norm{c-a^i} + \norm{a^i-a^i_\beta} \\
    \implies \norm{c-a^i} &\leq \norm{c-a^i_\beta} - \norm{a^i-a^i_\beta} \\
    &= r-\norm{a^i-a^i_\beta} \\
    &\leq r
\end{align*}
since $\norm{a^i-a^i_\beta}\geq0$. Hence $\norm{c-a^i}\leq r$ for $i=1,\ldots,n$ and so $B(c, r)$ is feasible for $(P$).
\end{proof}

\begin{algorithm}[H]\label{fig:dcssh}
    \KwIn{Data set $\A$, Ball $B(\hat{c},\hat{r})$, scalar $s\geq1$}
    \KwOut{Ball $B(c,r)$}
    $\hat{a}:=\arg\max_{a\in\A}\norm{\hat{c}-a}$\;
    $\beta:=\hat{c}-\hat{a}$\;
    $\gamma:=\hat{r}^2$\;
    \For{$i=1,\ldots,n$}{
        $\alpha_i:=a^i-\hat{c}$\;
        $x_i:=Q(\alpha_i, \beta, \gamma)$\;
    }
    $x:=\min\left\{x_i: i=1,\ldots,n\right\}$\;
    $c:= \hat{c}-\beta x/s$\;
    $r:=\max_{i=1,\ldots,n}\norm{c-a^i}$\;
    \KwRet{$B(c,r)$}\;
    \caption{Direction-Constrained Single Step Heuristic}
\end{algorithm}
\begin{figure}
    \centering
    \begin{tikzpicture}
        % lines
        \draw (0,0) -- (1.598, 1.5) node[midway, above left]{$\norm{c-a^i}$} -- (2.598, 1.5) -- cycle node[midway, below right]{$\norm{c-a^i_\beta}$};
        \draw[dotted] (2.598, 1.5) -- (3.598, 1.5);
        
        \draw[->, color=magenta] (-2,0) -- (0.95,0) node[pos=0.3333, below]{$\beta$};
        
        % points
        \draw[fill] (0,0) circle (1.5pt) node[below] {$c$};
        \draw[fill] (-2,0) circle (1.5pt) node[left]{$\hat{a}$};
        \draw[fill] (1,0) circle (1.5pt) node[below]{$\hat{c}$};
        
        \draw[fill] (1.598, 1.5) circle (1.5pt) node[above left]{$a^i$};
        \draw[fill] (2.598, 1.5) circle (1.5pt) node[above right]{$a^i_\beta$};
        \draw[fill] (3.598, 1.5) circle (1.5pt) node[above right]{$\hat{a}^i_\beta$};
        
        % balls
        \draw (0,0) circle (3);
        \draw[dashed] (1,0) circle (3);
        
        % labels
        \node at (2.598, 3.5) {$\norm{a^i-a^i_\beta}$};
        \draw[->] (2.598, 3) to[bend left=10] (2.098, 1.625);
        
    \end{tikzpicture}
    \caption{Visual Aid for Proposition \ref{imp heuristic feasibility}}
    \label{vis aid feasibility}
\end{figure}
\subsubsection{Complexity}

The complexity of the DCSSH is easy to determine. Referring to Algorithm \ref{fig:dcssh}, on line 1 we have a loop over each data point which runs in $O(n)$ time. On lines 4 through 6 we have a \texttt{for} loop which runs $n$ many times, with the operations on lines 5 and 6 both being $O(d)$ as they are each single operations on $d$-dimensional vectors, so overall this loop runs in $O(nd)$ time. On lines 7 and 8 we find the minimum and maximum of two lists respectively, again $O(n)$. Finally, on line 8, a single operation on $d$-dimensional vectors runs in $O(d)$ time. Our dominating term is then $O(nd)$.
\subsection{Direction-Constrained MEB Heuristic}
\subsubsection{Derivation}
For a visual aid to this derivation please refer to Figure \ref{fig:dcmeb}. Consider $B(\hat{c},\hat{r})$. Similarly to the DCSSH, for an improving direction we choose the furthest point in $\A$ from $\hat{c}$, $\hat{a}=\arg\max_{a\in\A}\norm{\hat{c}-a}$, and let $\beta=\hat{a}-\hat{c}$ be this direction. Note that this is the negative of $\beta$ used in the derivation of the DCSSH, as we are now considering the direction $\hat{c}\to\hat{a}$. Now, instead of calculating distances between each point in data and the surface of the ball to find a feasible step length from $\hat{c}$ towards $\hat{a}$, we formulate an optimization model to find an optimal step length such that the new ball still contains all data, but has a smaller radius.

We modify the optimization model for the MEB problem, adding the constraint that $c$ must lie on the direction $\beta$ from $\hat{c}$. It is possible to formulate this exactly using the constraint $c=x(\hat{c}+\beta)$, however a better formulation is to consider a variable $x\in\mathbb{R}^{\geq0}$, then write $c=\hat{c}+\beta x$. Obviously $\hat{c}$ is a constant vector so this reduces the problem of finding the new $c$ to that of finding the optimal solution to one variable $x$, as opposed to $d$ many decision variables for each element of the vector $c$. The optimization model for the DCMEB problem is as follows:
\begin{center}
    \begin{tabular}{CCC}
        \displaystyle\min_{x,r} & r \\
        \text{s.t.} & \norm{\hat{c}+\beta x - a^i} \leq r & i=1,\ldots,n \\
        & x\geq0
    \end{tabular}
\end{center}

By setting $\gamma=r^2$, $\alpha^i=\hat{c}-a^i$ for $i=1,\ldots,n$, and squaring the norm constraints, we get the following QCQP:
\begin{center}
    \begin{tabular}{CCC}
        \displaystyle\min_{x,\gamma} & \gamma \\
        \text{s.t.} & \beta^T\beta x^2 + 2\alpha_i^T\beta x + \alpha_i^T\alpha_i \leq \gamma & i=1,\ldots,n \\
        & x\geq0
    \end{tabular}
\end{center}

This model has $n+1$ constraints and most crucially, 2 decision variables. This makes the model extremely computationally easy to solve by modern solvers such as Gurobi. In fact, like the original MEB problem, we may formulate the DCMEB problem as as a SOCP by considering the cones
\begin{equation*}
    C_i = \left\{(r,\beta x+\hat{c}-a^i) \in \mathbb{R}^{d+1}: \norm{\beta x +\hat{c}-a^i}\leq\gamma\right\}.
\end{equation*}
and as such the DCMEB problem may be solved by interior-point methods \cite{socp_ipm}. The DCMEB heuristic is then to simply solve the DCMEB, given a data set $\A$ and initial ball $B(\hat{c},\hat{r}$, using the solutions of the solved DCMEB problem for the new ball.

\begin{algorithm}[H]
    \KwIn{Data set $\A$, Ball $B(\hat{c},\hat{r}$)}
    \KwOut{Ball $B(c,r)$}
    $\hat{a}:=\arg\max_{a\in\A}\norm{\hat{c}-a}$\;
    Let $x$, $r$ be the solutions returned by solver for the DCMEB problem for $B(\hat{c},\hat{r})$\;
    $c:=c+x(\hat{a}-\hat{c})$\;
    \KwRet{$B(c,r)$}\;
    \caption{Direction-Constrained MEB Heuristic}
\end{algorithm}

\begin{figure}
    \centering
    \begin{tikzpicture}
        % lines
        \draw[->, color=magenta] (0,0) -- (-2.95,0) node[pos=0.6666, above]{$\beta$};
        \draw[dashed] (-1,0) -- (1.598, 1.5) node[midway, above left]{$\norm{\hat{c}+\beta x - a^i}$};
        % points
        \draw[fill] (0,0) circle (1.5pt) node[below]{$\hat{c}$};
        \draw[fill] (-3,0) circle (1.5pt) node[left]{$\hat{a}$};
        \draw[fill] (1.598, 1.5) circle (1.5pt) node[above]{$a^i$};
        \draw[fill] (-1,0) circle(1.5pt) node[below]{$\hat{c}+\beta x$};
        
        % ball
        \draw (0,0) circle (3);
    \end{tikzpicture}
    \caption{Visual Aid for Derivation of the Direction-Constrained MEB}
    \label{fig:dcmeb}
\end{figure}

\subsubsection{Complexity}

The operations in this heuristic are simply finding a maximally distant point which runs in $O(n)$ time, solving the DCMEB problem, and an assignment which runs in $O(1)$ time. So, the interest is in the time complexity of the method used to solve the DCMEB problem. As this problem is a SOCP it makes sense to apply interior-point methods which may then solve it in polynomial time (TODO: NEEDS CITATION).
% =============================================================================
% Chapter 4
% =============================================================================
\chapter{Implementation and Experiments}\label{implementation}
\section{Code Implementation}
We implement our algorithms and data generation using Python 3.8 \cite{python}. Python is a high-level procedural programming language which is commonly used in data science and mathematics for its low barrier to entry, wide-scale adoption, and rich package ecosystem. One drawback of using Python is that it is known to be slow compared to other languages. An implementation of our work in a faster programming language such as C (TODO: CITE C) will almost certainly result in faster run times --- in fact many Python packages such as sk-learn (TODO: CITE SOURCE and find another package which uses C) utilize C to run the more time-consuming processes, and the Gurobi \cite{gurobi} solver is written in C while Python and other languages with Gurobi support are merely interfaces to this solver. A new up-and-coming programming language is Julia (TODO: CITE JULIA) which promises similar run times to C (TODO: CITE) and has a mature library for interfacing with optimization solvers known as JuMP (TODO: CITE). Due to the time constraints on this paper, we make use of the author's prior knowledge in Python for the implementation as previously mentioned but again acknowledge that there are indeed potentially superior choices for this type of work.

Packages within Python we make use of in the implementation are NumPy \cite{numpy} for numerical computation and linear algebra, pandas \cite{pandasjeff_reback_2021_5060318}{pandasmckinney-proc-scipy-2010} for data manipulation, MatPlotLib \cite{matplotlib} and Seaborn \cite{seaborn} for visualisation and plotting, and finally TensorFlow Datasets \cite{TFDS} to access the MNIST \cite{lecun2010mnist} data set. (TODO: CHECK SPELLING AND GRAMMAR)

TODO: talk about choice of solver (Gurobi)

\section{Data}\label{data}
In this section we present and discuss the types of data we will use for bench-marking our algorithms for solving the MEBwO problem. Functions written and used for this project can be found in the script \texttt{src/data/generation.py}.

\subsubsection{On Random Number Generation}
In order to generate our data, we must have some method of creating random numbers. To generate uniform random variates from the distribution $U(0,1)$, the method of linear congruential generators (TODO: CITE) is well known within the literature, and provide us a computationally trivial way of generating pseudo-random uniform numbers.

With a source of pseudo-random uniform numbers, it is then possible to create a wide variety of different random variates from various distributions. For example, to sample a random variate from the distribution $U(a,b)$ where $a>b$, one must simply generate a random variate $U$, from the distribution $U(0,1)$, then $U(b-a)+a$ follows the distribution $U(a,b)$. To generate standard normal random variates, from (CITE: GENERATING NORMAL RANDOM VARIABLES) we can first generate $U_1$, $U_2$ then

Using linear congruential generators and then applying transformations to obtain other random variates allows the user direct control over the randomness in their computations, allowing for very easy reproducability. In our implementation, we instead use the \texttt{random} package from NumPy \cite{numpy}. This allows us to generate uniform random numbers using \texttt{np.random.uniform} and \texttt{np.random.normal} easily, and we may set a seed for the pseudo-random generator using \texttt{np.random.seed} for reproducability.


\subsection{Normal}
Our first data set is constructed by generating standard normal variates, that is random numbers from the normal distribution $X\sim N(0,1)$ with mean $0$ and variance $1$. Many examples of real-life data have been shown to be normally distributed and the normal distribution is symmetric around its mean in each dimension, so it is an obvious choice to be fit with an MEBwO.

This can be easily generated using NumPy \cite{numpy} by calling the function \texttt{np.random.normal(0, 1, (n,d))} to generate $n$ many $d$-dimensional standard normal vectors. See Figure \ref{fig:normal} for an example.

\datafigure{normal_2d.png}{normal_3d.png}{Example of Standard Normal Data}{fig:normal}

\subsection{Uniform Ball}\label{uniform ball}
Our next data type consists of points sampled uniformly from a hypersphere of dimension $d$, and in this paper we will specifically choose to uniformly sample points within the unit ball centered on the zero vector, $B(\mathbf{0},1)\subseteq\mathbb{R}^d$. See Figure \ref{fig:unifball} for an example. Like with the choice of normal data, spherical data such as that of a uniform ball is a reasonable choice for the MEBwO problem.

\datafigure{uniform_ball_2d.png}{uniform_ball_3d.png}{Example of a Uniform Ball}{fig:unifball}

To generate points uniformly within a unit ball, we must first generate points uniformly on the unit hypersphere. \cite{hyperspheresurface} provides a fast and easy to implement method of doing this, by first generating the standard normal vector $\mathbf{x}=(x_1,\ldots,x_d)$ where $x_i\sim N(0,1)$ for $i=1,\ldots,d$, and then the distribution of the normalized vectors $\mathbf{x}/\norm{\mathbf{x}}$ is uniform over the hypersphere of dimension $d$.

A naive approach would then be to multiply the point on the surface of the hypersphere by a uniform random variate $U\sim U(0,1)$ to obtain a point inside the unit ball, and indeed this does generate points within the unit ball, but they will not be uniformly distributed and points will be more densely located near the centre of the ball. Instead, from \cite{eldredge} we find that we can instead multiply our point on the surface of the hypersphere by $U^{1/d}$ to have our points be uniformly distributed within the hypersphere. More generally, multiplying our point on the surface by $r\cdot U^{1/d}$ for some radius $r>0$ gives us a point within the hypersphere of radius $r$. See Algorithm \ref{alg:unitball} for a detailed outline of this method.

\begin{algorithm}[H]
    \KwIn{Dimension $d\in\mathbb{N}$, radius $r\in\mathbb{R}^{\geq0}$, centre $c\in\mathbb{R}^d$}
    \KwOut{Point $x\in\mathbb{R}^d$}
    Generate $x:=(x_1,\ldots,x_d)$ where $x_i\sim N(0,1)$\;
    $x:=x/\norm{x}$\;
    Generate $U\sim U(0,1)$\;
    $x:=r\cdot x\cdot \left(U^{1/d}\right)$\;
    $x:= x+c$\;
    \KwRet{x}
    
    \caption{Algorithm for Generating Points in a Hypersphere of Radius $r$}
    \label{alg:unitball}
\end{algorithm}

One benefit of using this data is that when we are bench-marking the Relaxation-Based Heuristic, we do not need to spend time calculating a lower bound for $M$. Since we know our data set is contained within a unit ball, our diameter can be at most 2. The true diameter of any given uniform ball data set will be some value marginally close to 2, but crucially, less than 2. This means an $M$ value of 2 will ensure that our big $M$ constraints work correctly, and the closeness of the true diameter to 2 means that the increase in solution time for the exact model is negligible.

\subsection{Hyperspherical Shell}
Now we will discuss the data type we call the Hyperspherical Shell. This consists of points sampled uniformly between the surfaces of two concentric hyperspheres with the same centres where one is strictly contained within the other. We may explicitly define this region as
\begin{equation*}
    H(c,r_1,r_2) = \left\{x\in\mathbb{R}^d: r_1 \leq \norm{x-c} \leq r_2\right\}
\end{equation*}

In this paper we choose the zero vector as our center, an inner radius of $1$, and an outer radius of $2$. See Figure \ref{fig:hypshell} for an example. In two dimensions the shape approximated by this data is known as an annulus, and in three dimensions a spherical shell, so for the general $d$-dimensional case we opt to call it a hyperspherical shell.

\datafigure{hyperspherical_shell_2d.png}{hyperspherical_shell_3d.png}{Example of a Hyperspherical Shell}{fig:hypshell}

This data type is chosen as the optimal centre of the MEBwO on any sufficiently large data set is guaranteed to be ``far" away from any point in the data set. This poses issues for Shenmaier's Approximation which always returns a centre which is a point within the data set.

To generate points within this set, we can make use of Algorithm \ref{alg:unitball} to generate points uniformly in a hypersphere whose radius is our chosen outer radius, then use an acceptance-rejection method to reject points whose distance from our chosen centre is less than our chosen inner radius. A common drawback of acceptance-rejection methods is that of efficiency, i.e. of the space in which we are generating points, what percentage of this space is our acceptance region. Depending on the amount of points desired, a low efficiency can result in very long computation times due to the number of points being rejected.

Acceptance-rejection methods are usually unsuitable for generating high-dimensional data, as a result of the ``curse of dimensionality". For example, if we wanted to generate points within a unit ball using an acceptance-rejection method, the ratio of the volume of a hypersphere inscribed within a hypercube to the volume of that hypercube tends to $0$ as $d\to\infty$ \cite{curse}.

Fortunately, in our case, the ratio of the acceptance region to the total volume of the hypersphere tends to $1$ as $d\to\infty$. To see this, note that the volume of a $d$-dimensional hypersphere with radius $r$ is
\begin{equation*}
    V_d(r) = \frac{S_d r^d}{d}
\end{equation*}
where $S_d$ is the hyper-surface area of the unit hypersphere in $d$ dimensions \cite{hypersphere}. Now, for inner radius $r_1$ and outer radius $r_2$ where $r_2>r_1$, we have
\begin{align*}
    \frac{V_d(r_1)}{V_d(r_2)} &= \frac{\left(\frac{S_dr_1^d}{d}\right)}{\left(\frac{S_dr_2^d}{d}\right)} \\
    &= \frac{r_1^d}{r_2^d} \\
    &= \left(\frac{r_1}{r_2}\right)^d
\end{align*}
which tends to $0$ as $d\to\infty$ since $r_2>r_1$. So, for sufficiently large $d$, the efficiency of this acceptance-rejection method is close to $1$. We outline our acceptance-rejection method in Algorithm \ref{alg:shell}

\begin{algorithm}
    \KwIn{Dimension $d\in\mathbb{N}$, inner radius $r_1\in\mathbb{R}^{\geq0}$, outer radius $r_2\in\mathbb{R}^{\geq0}$, center $c\in\mathbb{R}^d$}
    \KwOut{Point $x\in\mathbb{R}^d$}
    
    \While{True}{
        Generate $x$ from Algorithm \ref{alg:unitball} with $r=r_2$\;
        \uIf{$\norm{x-c}>r_1$}{
            break\;
        }
    }
    \KwRet{x}\;
    
    \caption{Acceptance-Rejection Method for Generating Points in a Hyperspherical Shell}
    \label{alg:shell}
\end{algorithm}
\subsection{Uniform Ball with Outliers}
Our final randomly generated data set is the Uniform Ball with Outliers. This data type consists of a uniform ball with a set amount of ``outliers" which we generate ourselves. The main benefit of using this data type to benchmark our algorithms is that since we can choose the exact number of points outside the uniform ball, if we set $\eta\%$ of our $n$ points to be outside of the ball with radius $r$ then $\MEBwO(\A,\eta)$ will have a radius close to $r$.

To generate this data, we simply use Algorithm \ref{alg:unitball} to generate a uniform unit ball with radius $1$ which contains $\eta\%$ of our data, and then use Algorithm \ref{alg:shell} with inner radius $2$ and outer radius $3$ which contains $(1-\eta)\%$ of our data. We choose an inner radius of $2$ so that there is a distinct gap between the two parts of the data in the interest of having a clear inner ball.

% TODO: write algorithm for unif ball w outliers
\subsection{MNIST}

\section{Experiments}
\subsection{Methodology}
\subsection{Exact Model}\label{exact benchmarks}
\subsection{Construction Methods}
\subsubsection{Relaxation-based Heuristic}
\subsubsection{Shrinking Heuristic}
\subsubsection{Shenmaier's Approximation}

\subsection{Improvement Heuristics}
\subsubsection{Direction-Constrained Single Step Heuristic}
\subsubsection{Direction-Constrained MEB}

\section{Discussion}

% =============================================================================
% Chapter 5
% =============================================================================
\chapter{Conclusion}

% =============================================================================
%the entries have to be in the file literature.bib
\bibliography{literature}
\clearpage

\appendix
\section*{Appendices}
\addcontentsline{toc}{section}{Appendices}

\section{An Appendix}
\label{app:one}

Some stuff.
\clearpage

\section{Another Appendix}
\label{app:two}

Some other stuff.

\end{document}
